# üß™ Manuel de Tests Automatis√©s : Framework Pytest et Pratiques Avanc√©es

## Pr√©face : L'Art du Test-Driven Development

Ce manuel constitue une r√©f√©rence compl√®te pour l'apprentissage et la ma√Ætrise des tests automatis√©s dans un contexte de d√©veloppement web moderne. Destin√© aux √©tudiants en informatique et aux d√©veloppeurs professionnels, il pr√©sente une approche rigoureuse des m√©thodologies de test, avec un focus particulier sur les architectures FastAPI et les int√©grations de services externes.

L'approche p√©dagogique combine th√©orie formelle, analyse de cas pratiques issus du projet MagikSwipe, et exercices progressifs permettant une assimilation graduelle des concepts.

---

## Chapitre I : Fondements Th√©oriques des Tests Automatis√©s

### 1.1 D√©finition et Classification des Tests

Un **test automatis√©** constitue une proc√©dure algorithmique permettant de valider le comportement fonctionnel d'un syst√®me logiciel de mani√®re r√©p√©table et objective. Contrairement aux tests manuels, les tests automatis√©s assurent une couverture syst√©matique et √©liminent les variations introduites par l'op√©rateur humain.

#### Taxonomie des Tests Selon l'IEEE 829

**Tests fonctionnels :**
- **Tests unitaires** : Validation des composants individuels
- **Tests d'int√©gration** : Validation des interactions inter-composants
- **Tests syst√®me** : Validation du syst√®me complet
- **Tests d'acceptation** : Validation des exigences m√©tier

**Tests non-fonctionnels :**
- **Tests de performance** : M√©triques de rapidit√© et scalabilit√©
- **Tests de s√©curit√©** : Validation des contr√¥les d'acc√®s
- **Tests de robustesse** : Gestion des conditions d'erreur

### 1.2 Justification √âconomique et Technique

#### 1.2.1 R√©duction du Co√ªt du Cycle de D√©veloppement

L'analyse quantitative d√©montre que la d√©tection pr√©coce des anomalies r√©duit significativement les co√ªts de correction :

- **Phase de sp√©cification** : Co√ªt unitaire = 1x
- **Phase de d√©veloppement** : Co√ªt unitaire = 6x
- **Phase de test** : Co√ªt unitaire = 15x
- **Phase de production** : Co√ªt unitaire = 60x

#### 1.2.2 Am√©lioration de la Qualit√© Logicielle

Les m√©triques de qualit√© logicielle √©tablissent une corr√©lation directe entre couverture de test et fiabilit√© :

- **Couverture < 50%** : Risque √©lev√© de r√©gression
- **Couverture 50-80%** : Qualit√© acceptable pour applications critiques
- **Couverture > 80%** : Qualit√© optimale avec maintenance facilit√©e

#### 1.2.3 Acc√©l√©ration des Cycles de Release

L'automatisation des tests permet :
- **Int√©gration continue** : Validation automatique √† chaque commit
- **D√©ploiement continu** : R√©duction des fen√™tres de maintenance
- **Refactoring s√©curis√©** : Modification du code sans risque de r√©gression

### 1.3 M√©thodologies de Test

#### 1.3.1 Test-Driven Development (TDD)

Le TDD impose un cycle rigoureux : **Rouge ‚Üí Vert ‚Üí Refactor**

```python
# Phase ROUGE : √âchec attendu
def test_slugify_empty_string():
    assert slugify("") == "invalid-input"

# Phase VERT : Impl√©mentation minimale
def slugify(text):
    if not text:
        return "invalid-input"
    return text.lower().replace(" ", "-")

# Phase REFACTOR : Optimisation
def slugify(text):
    return text.lower().strip().replace(" ", "-") if text else ""
```

#### 1.3.2 Behavior-Driven Development (BDD)

Le BDD √©tend le TDD en int√©grant le langage m√©tier :

```gherkin
Sc√©nario: Cr√©ation d'un univers multilingue
  √âtant donn√© un utilisateur authentifi√©
  Quand il cr√©e un univers "Magical Forest"
  Et qu'il d√©finit les langues ["fr", "en", "es"]
  Alors l'univers est cr√©√© avec succ√®s
  Et les traductions sont initialis√©es
```

#### 1.3.3 Property-Based Testing

Au-del√† des exemples sp√©cifiques, validation des propri√©t√©s invariantes :

```python
from hypothesis import given, strategies as st

@given(st.text())
def test_slugify_idempotent(text):
    """Propri√©t√© : slugify(slugify(x)) = slugify(x)"""
    result = slugify(text)
    assert slugify(result) == result

@given(st.text(), st.text())
def test_slugify_concatenation(text1, text2):
    """Propri√©t√© : slugify(a + b) ‚â† slugify(a) + slugify(b)"""
    combined = slugify(text1 + " " + text2)
    separate = slugify(text1) + "-" + slugify(text2)
    # Propri√©t√© attendue : normalisation diff√©rente
    assert isinstance(combined, str)
    assert isinstance(separate, str)
```

---

## Chapitre II : Architecture des Tests dans FastAPI

### 2.1 Patterns de Test pour Applications Web

#### 2.1.1 TestClient Pattern

Dans les architectures FastAPI, le `TestClient` constitue l'abstraction principale pour les tests d'int√©gration :

```python
from fastapi.testclient import TestClient
from main import app

def test_universe_creation_endpoint():
    """Test d'int√©gration pour endpoint de cr√©ation d'univers."""
    client = TestClient(app)

    # Pr√©paration des donn√©es de test
    payload = {
        "name": "Test Universe",
        "slug": "test-universe",
        "is_public": True
    }

    # Ex√©cution de la requ√™te
    response = client.post("/api/universes", json=payload)

    # Validation de la r√©ponse
    assert response.status_code == 201
    data = response.json()

    # Assertions m√©tier
    assert data["name"] == payload["name"]
    assert data["is_public"] == True
    assert "id" in data
    assert "created_at" in data
```

#### 2.1.2 Dependency Injection Override

Pour les tests n√©cessitant un contr√¥le fin des d√©pendances :

```python
from fastapi import Depends
from sqlalchemy.orm import Session

# Fonction originale avec injection
def create_universe(data: UniversCreate, db: Session = Depends(get_db)):
    # Logique m√©tier
    pass

# Test avec override de d√©pendance
def test_create_universe_with_mocked_db():
    # Mock de la session DB
    mock_db = MagicMock()

    # Override de la d√©pendance
    app.dependency_overrides[get_db] = lambda: mock_db

    client = TestClient(app)
    response = client.post("/api/universes", json={...})

    # V√©rification des appels √† la DB
    mock_db.add.assert_called_once()
    mock_db.commit.assert_called_once()
```

### 2.2 Gestion des √âtats de Test

#### 2.2.1 Isolation des Tests de Base de Donn√©es

```python
@pytest.fixture(scope="function")
def db_session():
    """Session de base de donn√©es isol√©e pour tests."""
    # Cr√©ation d'une base en m√©moire
    engine = create_engine("sqlite:///:memory:")
    Base.metadata.create_all(bind=engine)

    SessionLocal = sessionmaker(bind=engine)
    session = SessionLocal()

    yield session

    # Nettoyage post-test
    session.close()
    Base.metadata.drop_all(bind=engine)
```

#### 2.2.2 Fixtures Hi√©rarchiques

```python
@pytest.fixture(scope="session")
def app_client():
    """Client FastAPI global."""
    return TestClient(app)

@pytest.fixture(scope="function")
def authenticated_client(app_client, test_user):
    """Client avec authentification."""
    # Ajout du token d'authentification
    app_client.headers.update({"Authorization": f"Bearer {test_user.token}"})
    return app_client

@pytest.fixture(scope="function")
def test_universe(authenticated_client):
    """Univers de test cr√©√© via API."""
    response = authenticated_client.post("/api/universes", json={
        "name": "Test Universe",
        "slug": f"test-{uuid.uuid4().hex[:8]}"
    })
    return response.json()
```

### 2.3 Strat√©gies de Mocking

#### 2.3.1 Mocking des Services Externes

```python
from unittest.mock import patch, MagicMock

@patch('services.replicate_service.ReplicateClient.generate')
def test_music_generation_with_mock(mock_generate, client, test_universe):
    """Test de g√©n√©ration musicale avec service mock√©."""

    # Configuration du mock
    mock_response = {
        "audio_url": "https://example.com/generated-music.mp3",
        "duration": 60
    }
    mock_generate.return_value = mock_response

    # Ex√©cution du test
    response = client.post(f"/api/generate/{test_universe['slug']}/music", json={
        "language": "fr"
    })

    # Assertions
    assert response.status_code == 200
    mock_generate.assert_called_once_with(
        prompt="musique douce enfantine",
        duration=60
    )
```

#### 2.3.2 Mocking des Modules Externes

```python
@patch('services.generation_service.GoogleTranslator')
def test_translation_service_mock(mock_translator, client):
    """Test du service de traduction avec mock."""

    # Configuration du mock translator
    mock_instance = MagicMock()
    mock_instance.translate.return_value = "Hello World"
    mock_translator.return_value = mock_instance

    # Test de traduction
    response = client.post("/api/translate", json={
        "text": "Bonjour le monde",
        "target_lang": "en"
    })

    assert response.status_code == 200
    assert response.json()["translated"] == "Hello World"
    mock_instance.translate.assert_called_once_with("Bonjour le monde", dest="en")
```

### üèóÔ∏è Les Types de Tests

#### **1. Tests Unitaires** (Les plus importants)
Testent UNE fonction/un composant isol√© :
```python
def test_slugify():
    assert slugify("Hello World") == "hello-world"
    assert slugify("Ceci est un TEST") == "ceci-est-un-test"
```

#### **2. Tests d'Int√©gration**
Testent l'interaction entre composants :
```python
def test_user_creation_with_database():
    # Test que la cr√©ation d'utilisateur sauvegarde en DB
    user = create_user_in_db("test@test.com")
    saved_user = get_user_from_db(user.id)
    assert saved_user.email == "test@test.com"
```

#### **3. Tests End-to-End (E2E)**
Testent l'application compl√®te :
```python
def test_full_user_registration_flow():
    # Simule un utilisateur qui s'inscrit via l'API
    response = client.post("/register", json={"email": "user@test.com"})
    assert response.status_code == 201
    # V√©rifie que l'email de confirmation est envoy√©, etc.
```

---

## Chapitre III : Framework Pytest - Analyse Technique D√©taill√©e

### 3.1 Architecture et Philosophie de Pytest

Pytest constitue une √©volution majeure du framework de test `unittest` natif Python, introduisant une approche d√©clarative et extensible. Son architecture repose sur trois principes fondamentaux :

#### 3.1.1 D√©couverte Automatique des Tests

Pytest impl√©mente un algorithme sophistiqu√© de d√©couverte automatique :

```python
# Convention de nommage
def test_*()          # Fonctions de test
class Test*           # Classes de test
test_*.py            # Fichiers de test

# D√©couverte r√©cursive
tests/
‚îú‚îÄ‚îÄ test_api.py
‚îú‚îÄ‚îÄ integration/
‚îÇ   ‚îî‚îÄ‚îÄ test_database.py
‚îî‚îÄ‚îÄ unit/
    ‚îî‚îÄ‚îÄ test_models.py
```

#### 3.1.2 Syst√®me d'Assertions Avanc√©

Au-del√† des assertions simples, Pytest offre un syst√®me d'introspection :

```python
# Assertions avec contexte enrichi
def test_universe_creation():
    response = client.post("/api/universes", json={"name": ""})

    # Assertion avec message contextuel
    assert response.status_code == 422, f"Expected validation error, got {response.status_code}"

    # Validation de la structure d'erreur
    error = response.json()
    assert "detail" in error
    assert any("name" in str(field) for field in error.get("detail", []))
```

#### 3.1.3 Gestion Fine du Cycle de Vie

```python
@pytest.fixture(scope="session", autouse=True)
def setup_test_environment():
    """Configuration globale des tests."""
    # Initialisation de la base de donn√©es de test
    init_test_database()

    yield

    # Nettoyage post-session
    cleanup_test_database()

@pytest.fixture(scope="function")
def authenticated_client(client, test_user):
    """Client avec session authentifi√©e."""
    # Injection du token JWT
    client.headers.update({
        "Authorization": f"Bearer {test_user['access_token']}"
    })
    return client
```

### 3.2 Param√©trage et G√©n√©ration de Cas de Test

#### 3.2.1 Param√©trage Simple

```python
@pytest.mark.parametrize("language,expected_count", [
    ("fr", 5),  # Concepts en fran√ßais
    ("en", 5),  # Concepts en anglais
    ("es", 4),  # Concepts en espagnol
    ("invalid", 0),  # Langue non support√©e
])
def test_music_prompt_creation_by_language(client, test_universe, language, expected_count):
    """Test param√©tris√© de cr√©ation de prompts musicaux."""
    slug = test_universe["slug"]

    # Tentative de cr√©ation
    response = client.post(f"/api/universes/{slug}/music-prompts", json={
        "language": language,
        "prompt": f"Musique en {language}",
        "lyrics": f"Paroles en {language}"
    })

    if language in ["fr", "en", "es", "it", "de"]:
        assert response.status_code == 201
        # Validation suppl√©mentaire selon la langue
    else:
        assert response.status_code == 422  # Validation √©choue
```

#### 3.2.2 Param√©trage avec Fixtures

```python
@pytest.fixture(params=[
    {"theme": "animaux", "count": 3, "language": "fr"},
    {"theme": "couleurs", "count": 5, "language": "en"},
    {"theme": "chiffres", "count": 10, "language": "es"}
])
def concept_generation_params(request):
    return request.param

def test_concept_generation_parameterized(client, test_universe, concept_generation_params):
    """Test param√©tris√© de g√©n√©ration de concepts."""
    slug = test_universe["slug"]
    params = concept_generation_params

    response = client.post(f"/api/generate/{slug}/concepts", json=params)

    if params["language"] in ["fr", "en", "es"]:
        assert response.status_code == 200
        data = response.json()
        assert len(data["concepts"]) == params["count"]
    else:
        assert response.status_code == 422
```

### 3.3 Gestion Avanc√©e des Erreurs et Exceptions

#### 3.3.1 Assertions sur les Exceptions

```python
def test_universe_slug_validation():
    """Test de validation des slugs d'univers."""

    # Cas valides
    valid_slugs = ["test-universe", "my_universe_123", "univers-magique"]
    for slug in valid_slugs:
        assert is_valid_slug(slug) == True

    # Cas invalides
    with pytest.raises(ValueError, match="Slug contains invalid characters"):
        validate_universe_slug("test universe")  # Espace

    with pytest.raises(ValueError, match="Slug too short"):
        validate_universe_slug("a")  # Trop court

    with pytest.raises(ValueError, match="Slug already exists"):
        create_universe_with_slug("existing-slug")
```

#### 3.3.2 Gestion des Warnings et Deprecation

```python
import warnings

def test_deprecated_api_warnings(client):
    """Test des avertissements pour API d√©pr√©ci√©e."""

    with warnings.catch_warnings(record=True) as w:
        warnings.simplefilter("always")

        # Appel √† une API d√©pr√©ci√©e
        response = client.get("/api/deprecated/endpoint")

        # V√©rification des avertissements
        assert len(w) == 1
        assert issubclass(w[0].category, DeprecationWarning)
        assert "deprecated" in str(w[0].message).lower()
```

### 3.4 Int√©gration Continue et M√©triques

#### 3.4.1 Configuration pour CI/CD

```ini
# pytest.ini
[tool:pytest.ini_options]
testpaths = tests
python_files = test_*.py
addopts =
    --strict-markers
    --disable-warnings
    --cov=backend
    --cov-report=xml
    --cov-report=html
    --cov-fail-under=80
markers =
    unit: Tests unitaires rapides
    integration: Tests d'int√©gration
    slow: Tests lents (>30s)
    api: Tests d'API endpoints
```

#### 3.4.2 M√©triques de Qualit√©

```python
def test_code_coverage_metrics():
    """Validation des m√©triques de couverture."""
    # Import apr√®s ex√©cution des tests
    import coverage

    cov = coverage.Coverage()
    cov.load()

    # V√©rifications de couverture
    assert cov.report() > 80.0  # Couverture globale > 80%

    # Couverture par module critique
    for module in ["models", "routes", "services"]:
        module_cov = cov.report(include=f"backend/{module}/*")
        assert module_cov > 85.0, f"Couverture {module} insuffisante: {module_cov}%"
```

---

## Chapitre IV : Impl√©mentation Avanc√©e dans MagikSwipe

### 4.1 Architecture de Test du Projet

#### 4.1.1 Structure Hi√©rarchique des Tests

```
backend/tests/
‚îú‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ conftest.py                    # Configuration centralis√©e
‚îú‚îÄ‚îÄ fixtures/                      # Fixtures sp√©cialis√©es
‚îÇ   ‚îú‚îÄ‚îÄ database.py               # Fixtures DB
‚îÇ   ‚îú‚îÄ‚îÄ auth.py                   # Fixtures authentification
‚îÇ   ‚îî‚îÄ‚îÄ external_services.py      # Mocks pour services externes
‚îú‚îÄ‚îÄ unit/                         # Tests unitaires
‚îÇ   ‚îú‚îÄ‚îÄ test_models.py           # Validation mod√®les SQLAlchemy
‚îÇ   ‚îú‚îÄ‚îÄ test_schemas.py          # Validation Pydantic
‚îÇ   ‚îî‚îÄ‚îÄ test_utils.py            # Fonctions utilitaires
‚îú‚îÄ‚îÄ integration/                  # Tests d'int√©gration
‚îÇ   ‚îú‚îÄ‚îÄ test_database_ops.py     # Op√©rations DB complexes
‚îÇ   ‚îú‚îÄ‚îÄ test_api_flows.py        # Flux API complets
‚îÇ   ‚îî‚îÄ‚îÄ test_external_services.py # Int√©grations externes
‚îú‚îÄ‚îÄ api/                          # Tests API REST
‚îÇ   ‚îú‚îÄ‚îÄ test_universes.py        # Endpoints univers
‚îÇ   ‚îú‚îÄ‚îÄ test_assets.py           # Endpoints assets
‚îÇ   ‚îú‚îÄ‚îÄ test_music_prompts.py    # Endpoints musique
‚îÇ   ‚îú‚îÄ‚îÄ test_generation.py       # Endpoints g√©n√©ration
‚îÇ   ‚îî‚îÄ‚îÄ test_sync.py             # Endpoints synchronisation
‚îú‚îÄ‚îÄ e2e/                          # Tests end-to-end
‚îÇ   ‚îî‚îÄ‚îÄ test_user_journeys.py    # Parcours utilisateur complets
‚îú‚îÄ‚îÄ utils/                        # Utilitaires de test
‚îÇ   ‚îú‚îÄ‚îÄ factories.py             # Factories pour donn√©es de test
‚îÇ   ‚îú‚îÄ‚îÄ assertions.py            # Assertions personnalis√©es
‚îÇ   ‚îî‚îÄ‚îÄ helpers.py               # Fonctions helper
‚îú‚îÄ‚îÄ run_tests.py                  # Lanceur personnalis√©
‚îî‚îÄ‚îÄ README.md
```

#### 4.1.2 Configuration Avanc√©e de Pytest

```python
# conftest.py - Configuration centralis√©e
import os
import pytest
from fastapi.testclient import TestClient
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker

# Configuration selon l'environnement
TEST_DATABASE_URL = os.getenv("TEST_DATABASE_URL", "sqlite:///:memory:")
REPLICATE_MOCK_ENABLED = os.getenv("REPLICATE_MOCK_ENABLED", "true").lower() == "true"

@pytest.fixture(scope="session")
def test_engine():
    """Moteur de base de donn√©es de test."""
    engine = create_engine(TEST_DATABASE_URL, echo=False)

    # Configuration sp√©cifique aux tests
    if TEST_DATABASE_URL.startswith("sqlite"):
        # Optimisations SQLite pour tests
        engine.execute("PRAGMA foreign_keys = ON;")
        engine.execute("PRAGMA journal_mode = MEMORY;")

    return engine

@pytest.fixture(scope="session")
def test_db_setup(test_engine):
    """Configuration de la base de donn√©es de test."""
    from database.connection import Base
    Base.metadata.create_all(bind=test_engine)

    yield

    # Nettoyage complet post-session
    Base.metadata.drop_all(bind=test_engine)

@pytest.fixture(scope="function")
def db_session(test_engine, test_db_setup):
    """Session de base de donn√©es isol√©e."""
    SessionLocal = sessionmaker(bind=test_engine)
    session = SessionLocal()

    # D√©marrage de transaction
    session.begin()

    yield session

    # Rollback automatique (pas de modifications persistantes)
    session.rollback()
    session.close()

@pytest.fixture(scope="function")
def client(db_session):
    """Client FastAPI avec session DB isol√©e."""
    from main import app

    # Override de la d√©pendance DB
    def get_test_db():
        return db_session

    app.dependency_overrides = {}
    # Note: Dans une vraie impl√©mentation, overrider get_db

    with TestClient(app) as test_client:
        yield test_client

@pytest.fixture(scope="function")
def authenticated_client(client):
    """Client avec authentification JWT."""
    # Simulation d'authentification
    # Dans un vrai syst√®me, utiliser un token JWT valide
    client.headers.update({
        "Authorization": "Bearer test-token-123"
    })
    return client
```

### 4.2 Patterns de Test Avanc√©s

#### 4.2.1 Factory Pattern pour Donn√©es de Test

```python
# tests/utils/factories.py
from typing import Dict, Any
import uuid

class UniverseFactory:
    """Factory pour cr√©ation d'univers de test."""

    @staticmethod
    def create_universe_data(**overrides) -> Dict[str, Any]:
        """G√©n√®re des donn√©es d'univers avec valeurs par d√©faut."""
        base_data = {
            "name": f"Test Universe {uuid.uuid4().hex[:8]}",
            "slug": f"test-universe-{uuid.uuid4().hex[:8]}",
            "is_public": True,
            "background_color": "#1a1a2e"
        }
        return {**base_data, **overrides}

    @classmethod
    def create_via_api(cls, client, **overrides):
        """Cr√©e un univers via API et retourne la r√©ponse."""
        data = cls.create_universe_data(**overrides)
        response = client.post("/api/universes", json=data)
        return response

    @classmethod
    def create_private_universe(cls, client, **overrides):
        """Factory sp√©cialis√©e pour univers priv√©s."""
        return cls.create_via_api(client, is_public=False, **overrides)

class MusicPromptFactory:
    """Factory pour prompts musicaux multilingues."""

    SUPPORTED_LANGUAGES = ["fr", "en", "es", "it", "de"]

    @staticmethod
    def create_prompt_data(language: str, **overrides) -> Dict[str, Any]:
        """G√©n√®re des donn√©es de prompt musical."""
        if language not in cls.SUPPORTED_LANGUAGES:
            raise ValueError(f"Langue non support√©e: {language}")

        base_data = {
            "language": language,
            "prompt": f"Musique instrumentale {language}",
            "lyrics": f"Paroles en {language}"
        }
        return {**base_data, **overrides}

    @classmethod
    def create_all_languages(cls, client, universe_slug: str):
        """Cr√©e des prompts pour toutes les langues support√©es."""
        prompts = {}
        for lang in cls.SUPPORTED_LANGUAGES:
            data = cls.create_prompt_data(lang)
            response = client.post(f"/api/universes/{universe_slug}/music-prompts", json=data)
            if response.status_code == 201:
                prompts[lang] = response.json()
        return prompts
```

#### 4.2.2 Assertions Personnalis√©es

```python
# tests/utils/assertions.py
from typing import Dict, Any

class APIAssertions:
    """Assertions sp√©cialis√©es pour tests API."""

    @staticmethod
    def assert_universe_response(response_data: Dict[str, Any]):
        """Valide la structure d'une r√©ponse d'univers."""
        required_fields = ["id", "name", "slug", "is_public", "created_at"]
        for field in required_fields:
            assert field in response_data, f"Champ requis manquant: {field}"

        # Validation des types
        assert isinstance(response_data["id"], str)
        assert isinstance(response_data["name"], str)
        assert isinstance(response_data["is_public"], bool)
        assert "translations" in response_data
        assert "assets" in response_data
        assert "music_prompts" in response_data

    @staticmethod
    def assert_music_prompt_response(response_data: Dict[str, Any]):
        """Valide la structure d'une r√©ponse de prompt musical."""
        required_fields = ["id", "language", "prompt", "lyrics", "created_at"]
        for field in required_fields:
            assert field in response_data, f"Champ requis manquant: {field}"

        # Validation m√©tier
        assert response_data["language"] in ["fr", "en", "es", "it", "de"]
        assert len(response_data["prompt"]) > 0
        assert len(response_data["lyrics"]) > 0

    @staticmethod
    def assert_generation_job_response(response_data: Dict[str, Any]):
        """Valide la structure d'une r√©ponse de job de g√©n√©ration."""
        assert "id" in response_data
        assert "type" in response_data
        assert "status" in response_data
        assert response_data["type"] in ["generate_images", "generate_music", "generate_all"]
        assert response_data["status"] in ["pending", "running", "completed", "failed"]
```

#### 4.2.3 Context Managers pour Tests Complexes

```python
# tests/utils/helpers.py
from contextlib import contextmanager
import time

@contextmanager
def assert_execution_time(max_seconds: float):
    """Context manager pour v√©rifier le temps d'ex√©cution."""
    start_time = time.time()
    try:
        yield
    finally:
        execution_time = time.time() - start_time
        assert execution_time <= max_seconds, f"Ex√©cution trop lente: {execution_time:.2f}s > {max_seconds}s"

@contextmanager
def mock_external_services():
    """Context manager pour mocker tous les services externes."""
    from unittest.mock import patch, MagicMock

    mocks = {}

    # Mock Replicate
    mock_replicate = patch('services.generation_service.replicate.run')
    mocks['replicate'] = mock_replicate.start()
    mocks['replicate'].return_value = ["mocked", "response"]

    # Mock Supabase
    mock_supabase = patch('services.supabase_service.SupabaseService.get_univers_by_slug')
    mocks['supabase'] = mock_supabase.start()
    mocks['supabase'].return_value = {"id": "mock-id", "name": "Mock Universe"}

    try:
        yield mocks
    finally:
        for mock in mocks.values():
            mock.stop()

def wait_for_job_completion(client, job_id: str, timeout: int = 30):
    """Attend la completion d'un job de g√©n√©ration."""
    import time

    start_time = time.time()
    while time.time() - start_time < timeout:
        response = client.get(f"/api/jobs/{job_id}")
        if response.status_code == 200:
            job_data = response.json()
            if job_data["status"] in ["completed", "failed"]:
                return job_data
        time.sleep(1)

    raise TimeoutError(f"Job {job_id} n'a pas termin√© dans les {timeout}s")
```

### 4.3 Exemples de Tests Avanc√©s du Projet MagikSwipe

#### 4.3.1 Test de Workflow Complet

```python
# tests/e2e/test_user_journeys.py
import pytest
from tests.utils.helpers import assert_execution_time, mock_external_services

class TestUniverseCreationJourney:
    """Test end-to-end de cr√©ation d'univers multilingue."""

    @pytest.mark.e2e
    def test_complete_universe_creation_workflow(self, authenticated_client):
        """Test du workflow complet de cr√©ation d'univers."""

        with mock_external_services() as mocks:
            # √âtape 1: Cr√©ation de l'univers
            universe_data = {
                "name": "Univers Enchant√©",
                "slug": "univers-enchante",
                "is_public": True,
                "background_color": "#ff6b6b"
            }

            response = authenticated_client.post("/api/universes", json=universe_data)
            assert response.status_code == 201
            universe = response.json()

            # √âtape 2: Ajout de traductions
            translations = {
                "en": "Enchanted Universe",
                "es": "Universo Encantado",
                "fr": "Univers Enchant√©"
            }

            response = authenticated_client.patch(f"/api/universes/{universe['slug']}", json={
                "translations": translations
            })
            assert response.status_code == 200

            # √âtape 3: Cr√©ation de prompts musicaux multilingues
            languages = ["fr", "en", "es"]
            for lang in languages:
                prompt_data = {
                    "language": lang,
                    "prompt": f"Musique f√©erique en {lang}",
                    "lyrics": f"Paroles magiques en {lang}"
                }
                response = authenticated_client.post(f"/api/universes/{universe['slug']}/music-prompts", json=prompt_data)
                assert response.status_code == 201

            # √âtape 4: G√©n√©ration de concepts
            with assert_execution_time(5.0):  # Max 5 secondes
                response = authenticated_client.post(f"/api/generate/{universe['slug']}/concepts", json={
                    "theme": "cr√©atures magiques",
                    "count": 5,
                    "language": "fr"
                })
                assert response.status_code == 200

            # √âtape 5: V√©rification de l'√©tat final
            response = authenticated_client.get(f"/api/universes/{universe['slug']}")
            assert response.status_code == 200
            final_universe = response.json()

            # Assertions finales
            assert len(final_universe["translations"]) == 3
            assert len(final_universe["music_prompts"]) == 3
            assert final_universe["is_public"] == True
```

#### 4.3.2 Test de Performance et Charge

```python
# tests/integration/test_performance.py
import pytest
from tests.utils.helpers import assert_execution_time

class TestAPI_Performance:
    """Tests de performance des endpoints critiques."""

    @pytest.mark.performance
    @pytest.mark.parametrize("num_universes", [10, 50, 100])
    def test_universe_listing_performance(self, client, num_universes):
        """Test de performance du listing d'univers."""

        # Cr√©ation de N univers
        for i in range(num_universes):
            data = {
                "name": f"Performance Universe {i}",
                "slug": f"perf-universe-{i}",
                "is_public": True
            }
            response = client.post("/api/universes", json=data)
            assert response.status_code == 201

        # Test de performance du listing
        with assert_execution_time(2.0):  # Max 2 secondes pour 100 univers
            response = client.get("/api/universes")
            assert response.status_code == 200

            data = response.json()
            assert len(data["items"]) >= num_universes

    def test_concurrent_music_prompt_creation(self, client, test_universe):
        """Test de cr√©ation concurrente de prompts musicaux."""
        import threading
        import queue

        slug = test_universe["slug"]
        results = queue.Queue()

        def create_prompt(lang):
            """Fonction ex√©cut√©e dans un thread."""
            data = {
                "language": lang,
                "prompt": f"Musique {lang}",
                "lyrics": f"Paroles {lang}"
            }
            response = client.post(f"/api/universes/{slug}/music-prompts", json=data)
            results.put((lang, response.status_code))

        # Cr√©ation de threads pour chaque langue
        threads = []
        languages = ["fr", "en", "es", "it", "de"]

        for lang in languages:
            thread = threading.Thread(target=create_prompt, args=(lang,))
            threads.append(thread)
            thread.start()

        # Attente de completion
        for thread in threads:
            thread.join(timeout=5.0)

        # V√©rification des r√©sultats
        successful_creations = 0
        while not results.empty():
            lang, status = results.get()
            if status == 201:
                successful_creations += 1

        assert successful_creations == len(languages)
```

#### 4.3.3 Test de S√©curit√© et Validation

```python
# tests/integration/test_security.py
import pytest

class TestInputValidation:
    """Tests de s√©curit√© et validation des entr√©es."""

    @pytest.mark.security
    @pytest.mark.parametrize("malicious_input,expected_status", [
        ("<script>alert('xss')</script>", 422),  # XSS attempt
        ("../../../etc/passwd", 422),            # Path traversal
        ("a" * 1000, 422),                      # Buffer overflow attempt
        ("", 422),                              # Empty input
        ("normal-universe", 201),               # Valid input
    ])
    def test_universe_slug_validation(self, client, malicious_input, expected_status):
        """Test de validation des slugs d'univers contre attaques."""
        data = {
            "name": "Test Universe",
            "slug": malicious_input,
            "is_public": True
        }

        response = client.post("/api/universes", json=data)
        assert response.status_code == expected_status

        if expected_status == 422:
            error_data = response.json()
            assert "detail" in error_data

    def test_sql_injection_prevention(self, client):
        """Test de pr√©vention des injections SQL."""
        malicious_slugs = [
            "'; DROP TABLE univers; --",
            "' OR '1'='1",
            "\"; SELECT * FROM univers; --"
        ]

        for malicious_slug in malicious_slugs:
            data = {
                "name": "Test Universe",
                "slug": malicious_slug,
                "is_public": True
            }

            response = client.post("/api/universes", json=data)
            # Devrait √©chouer √† la validation, pas ex√©cuter du SQL
            assert response.status_code in [400, 422]

    def test_rate_limiting_simulation(self, client):
        """Test de simulation de rate limiting."""
        # Effectuer de nombreuses requ√™tes rapides
        import time

        start_time = time.time()
        request_count = 0

        # Simuler 100 requ√™tes en 1 seconde
        while time.time() - start_time < 1.0 and request_count < 100:
            response = client.get("/api/universes")
            if response.status_code == 200:
                request_count += 1

        # Dans un vrai syst√®me, certaines requ√™tes seraient limit√©es
        # Ici, on teste juste que l'API reste stable
        assert request_count > 10  # Au moins 10 requ√™tes r√©ussies
```
backend/tests/
‚îú‚îÄ‚îÄ __init__.py              # Package Python
‚îú‚îÄ‚îÄ conftest.py              # Configuration + fixtures globales
‚îú‚îÄ‚îÄ test_universes.py        # Tests CRUD univers
‚îú‚îÄ‚îÄ test_assets.py           # Tests CRUD assets
‚îú‚îÄ‚îÄ test_music_prompts.py    # Tests CRUD prompts musique
‚îú‚îÄ‚îÄ test_generation.py       # Tests g√©n√©ration IA (mocks)
‚îú‚îÄ‚îÄ test_jobs_sync.py        # Tests jobs + sync Supabase
‚îú‚îÄ‚îÄ run_tests.py             # Lanceur personnalis√©
‚îî‚îÄ‚îÄ README.md                # Documentation
```

### ‚öôÔ∏è Configuration (conftest.py)

```python
import pytest
from fastapi.testclient import TestClient
from main import app

@pytest.fixture(scope="function")
def client():
    """Client FastAPI pour les tests API."""
    with TestClient(app) as test_client:
        yield test_client

@pytest.fixture(scope="function")
def test_universe(client):
    """Cr√©e un univers de test unique."""
    import uuid
    unique_slug = f"test-universe-{uuid.uuid4().hex[:8]}"

    response = client.post("/api/universes", json={
        "name": "Test Universe",
        "slug": unique_slug,
        "is_public": True
    })
    assert response.status_code == 201
    return response.json()
```

### üöÄ Lancement des Tests

#### **Commandes de Base**
```bash
cd backend

# Tous les tests
pytest tests/

# Tests sp√©cifiques
pytest tests/test_universes.py
pytest tests/test_music_prompts.py::TestMusicPromptsCRUD::test_create_music_prompt_french

# Avec d√©tails
pytest -v

# Avec couverture
pytest --cov=backend --cov-report=html
```

#### **Notre Lanceur Personnalis√©**
```python
# run_tests.py
python run_tests.py --music        # Tests musique seulement
python run_tests.py --universes    # Tests univers seulement
```

---

## Chapitre V : Conception et Strat√©gies de Test

### 5.1 M√©thodologie de Conception de Tests

#### 5.1.1 Analyse des Risques et Priorisation

La conception de tests doit suivre une analyse rigoureuse des risques :

**Facteurs de criticit√© :**
- **Impact m√©tier** : Fonctionnalit√©s core vs. features secondaires
- **Fr√©quence d'usage** : Endpoints fr√©quemment utilis√©s
- **Complexit√© technique** : Logique m√©tier complexe
- **D√©pendances externes** : Int√©grations avec services tiers

**Matrice de priorisation pour MagikSwipe :**

| Fonctionnalit√© | Criticit√© | Tests Requis |
|---------------|-----------|--------------|
| CRUD Univers | √âlev√©e | Unitaires + Int√©gration + E2E |
| Gestion Visibilit√© | Moyenne | Int√©gration + S√©curit√© |
| Prompts Musique | Moyenne | Int√©gration + Validation |
| G√©n√©ration IA | √âlev√©e | Unitaires avec mocks |
| Synchronisation | √âlev√©e | Int√©gration + Fiabilit√© |

#### 5.1.2 Strat√©gie de Coverage

**Pyramide de test adapt√©e au contexte FastAPI :**

```
          /\
         /  \
    5% / E2E \
       /______\
      /        \
     /  INTEGRATION \
    /     20%        \
   /__________________\
  /                    \
 /     UNITAIRES        \
/        75%             \
--------------------------
```

**Rationale :**
- **Tests unitaires (75%)** : Logique m√©tier isol√©e, mod√®les, utilitaires
- **Tests d'int√©gration (20%)** : API endpoints, interactions DB, workflows
- **Tests E2E (5%)** : Parcours utilisateur critiques uniquement

### 5.2 Patterns de Test pour Applications Web

#### 5.2.1 Test des Endpoints REST

```python
def test_universe_crud_lifecycle(client):
    """Test complet du cycle de vie CRUD d'un univers."""

    # Phase 1: Cr√©ation
    create_data = {
        "name": "Test Universe",
        "slug": f"test-lifecycle-{uuid.uuid4().hex[:8]}",
        "is_public": True
    }

    create_response = client.post("/api/universes", json=create_data)
    assert create_response.status_code == 201
    universe = create_response.json()

    # Phase 2: Lecture
    read_response = client.get(f"/api/universes/{universe['slug']}")
    assert read_response.status_code == 200
    assert read_response.json()["name"] == create_data["name"]

    # Phase 3: Mise √† jour
    update_data = {"name": "Updated Universe Name"}
    update_response = client.patch(f"/api/universes/{universe['slug']}", json=update_data)
    assert update_response.status_code == 200
    assert update_response.json()["name"] == update_data["name"]

    # Phase 4: Suppression
    delete_response = client.delete(f"/api/universes/{universe['slug']}")
    assert delete_response.status_code == 204

    # Phase 5: V√©rification suppression
    final_response = client.get(f"/api/universes/{universe['slug']}")
    assert final_response.status_code == 404
```

#### 5.2.2 Test des Contraintes M√©tier

```python
def test_music_prompt_business_rules(client, test_universe):
    """Test des r√®gles m√©tier pour les prompts musicaux."""

    slug = test_universe["slug"]

    # R√®gle 1: Unicit√© par langue
    data_fr = {
        "language": "fr",
        "prompt": "Musique fran√ßaise",
        "lyrics": "Paroles fran√ßaises"
    }

    # Premi√®re cr√©ation r√©ussit
    response1 = client.post(f"/api/universes/{slug}/music-prompts", json=data_fr)
    assert response1.status_code == 201

    # Deuxi√®me cr√©ation pour m√™me langue √©choue
    response2 = client.post(f"/api/universes/{slug}/music-prompts", json=data_fr)
    assert response2.status_code == 400
    assert "already exists" in response2.json()["detail"]

    # R√®gle 2: Langues support√©es uniquement
    invalid_languages = ["jp", "ru", "zh", "invalid"]
    for lang in invalid_languages:
        data = {
            "language": lang,
            "prompt": f"Musique en {lang}",
            "lyrics": f"Paroles en {lang}"
        }
        response = client.post(f"/api/universes/{slug}/music-prompts", json=data)
        assert response.status_code == 422  # Validation Pydantic

    # R√®gle 3: Toutes les langues peuvent coexister
    valid_languages = ["fr", "en", "es", "it", "de"]
    for lang in valid_languages[1:]:  # fr d√©j√† cr√©√©
        data = {
            "language": lang,
            "prompt": f"Musique en {lang}",
            "lyrics": f"Paroles en {lang}"
        }
        response = client.post(f"/api/universes/{slug}/music-prompts", json=data)
        assert response.status_code == 201
```

#### 5.2.3 Test des Sc√©narios d'Erreur

```python
@pytest.mark.parametrize("invalid_payload,expected_error", [
    # Donn√©es manquantes
    ({"slug": "test"}, "Field required"),
    ({"name": "Test"}, "Field required"),

    # Types incorrects
    ({"name": 123, "slug": "test", "is_public": True}, "Input should be a valid string"),
    ({"name": "Test", "slug": "test", "is_public": "true"}, "Input should be a valid boolean"),

    # Valeurs invalides
    ({"name": "", "slug": "test", "is_public": True}, "String should have at least 1 character"),
    ({"name": "Test", "slug": "", "is_public": True}, "String should have at least 1 character"),

    # Contraintes m√©tier
    ({"name": "Test", "slug": "test@invalid", "is_public": True}, "Slug contains invalid characters"),
])
def test_universe_creation_validation(client, invalid_payload, expected_error):
    """Test de validation des donn√©es d'entr√©e pour cr√©ation d'univers."""

    response = client.post("/api/universes", json=invalid_payload)
    assert response.status_code == 422

    error_detail = response.json()
    assert "detail" in error_detail

    # V√©rifier que l'erreur contient le message attendu
    error_messages = []
    if isinstance(error_detail["detail"], list):
        for error in error_detail["detail"]:
            if isinstance(error, dict) and "msg" in error:
                error_messages.append(error["msg"])
    else:
        error_messages.append(str(error_detail["detail"]))

    assert any(expected_error.lower() in msg.lower() for msg in error_messages), \
           f"Expected error '{expected_error}' not found in {error_messages}"
```

### 5.3 Gestion Avanc√©e des Donn√©es de Test

#### 5.3.1 Fixtures Hi√©rarchiques avec Contexte

```python
@pytest.fixture(scope="function")
def multilingual_universe(client):
    """Univers avec configuration multilingue compl√®te."""
    # Cr√©ation de base
    universe_response = client.post("/api/universes", json={
        "name": "Multilingual Universe",
        "slug": f"multi-universe-{uuid.uuid4().hex[:8]}",
        "is_public": True
    })
    universe = universe_response.json()

    # Ajout de traductions
    translations = {
        "en": "Multilingual Universe",
        "es": "Universo Multiling√ºe",
        "fr": "Univers Multilingue",
        "it": "Universo Multilingue",
        "de": "Mehrsprachiges Universum"
    }

    client.patch(f"/api/universes/{universe['slug']}", json={
        "translations": translations
    })

    # Ajout de prompts musicaux
    languages = ["fr", "en", "es", "it", "de"]
    music_prompts = {}

    for lang in languages:
        prompt_data = {
            "language": lang,
            "prompt": f"Musique orchestrale en {lang}",
            "lyrics": f"Paroles po√©tiques en {lang}"
        }
        response = client.post(f"/api/universes/{universe['slug']}/music-prompts", json=prompt_data)
        music_prompts[lang] = response.json()

    return {
        "universe": universe,
        "translations": translations,
        "music_prompts": music_prompts
    }
```

#### 5.3.2 Tests Param√©tris√©s avec Contexte

```python
@pytest.mark.parametrize("language,prompt_theme,expected_style", [
    ("fr", "f√©√©rique", "musique douce et enfantine"),
    ("en", "magical", "soft and enchanting music"),
    ("es", "m√°gico", "m√∫sica suave y encantadora"),
    ("it", "fatato", "musica dolce e incantatrice"),
    ("de", "zauberhaft", "sanfte und bezaubernde Musik"),
])
def test_music_generation_multilingual(client, multilingual_universe, language, prompt_theme, expected_style):
    """Test de g√©n√©ration musicale multilingue param√©tris√©."""

    universe_slug = multilingual_universe["universe"]["slug"]

    # V√©rifier que le prompt existe
    prompt_response = client.get(f"/api/universes/{universe_slug}/music-prompts/{language}")
    assert prompt_response.status_code == 200
    existing_prompt = prompt_response.json()

    # G√©n√©rer avec le prompt existant
    with patch('services.generation_service.replicate.run') as mock_replicate:
        mock_replicate.return_value = ["mocked_audio_data"]

        generation_response = client.post(f"/api/generate/{universe_slug}/music", json={
            "language": language
        })

        # V√©rifications
        assert generation_response.status_code == 200
        job_data = generation_response.json()

        # V√©rifier que le mock a √©t√© appel√© avec les bonnes donn√©es
        mock_replicate.assert_called_once()
        call_args = mock_replicate.call_args[1]  # Arguments keyword

        # Le prompt devrait contenir le style attendu
        assert expected_style in call_args.get("input", {}).get("prompt", "")
```

### üé® Bonnes Pratiques

#### **1. Nommage des Tests**
```python
# ‚úÖ BON
def test_create_universe_with_valid_data()
def test_create_universe_fails_with_duplicate_slug()

# ‚ùå MAUVAIS
def test_create()
def test_universe()
```

#### **2. Un Test = Un Concept**
```python
# ‚úÖ UN test = UNE fonctionnalit√©
def test_user_can_login_with_correct_password()
def test_user_cannot_login_with_wrong_password()

# ‚ùå Trop de choses dans un test
def test_user_login()  # Teste tout en m√™me temps
```

#### **3. Assertions Claires**
```python
# ‚úÖ Clair et pr√©cis
assert response.status_code == 201
assert user.email == "alice@42.fr"

# ‚ùå Pas assez sp√©cifique
assert response.ok  # Vague
assert user.is_valid  # Qu'est-ce que "valid" ?
```

#### **4. Tests Ind√©pendants**
```python
# ‚úÖ Chaque test est isol√©
def test_create_user():
    # Cr√©e un user

def test_delete_user():
    # Cr√©e un autre user, le supprime

# ‚ùå Tests d√©pendants (probl√©matique)
def test_create_and_delete():
    # Cr√©e ET supprime dans le m√™me test
```

---

## Chapitre VI : D√©bogage et R√©solution Avanc√©e des Tests

### 6.1 Diagnostic Syst√©matique des √âchecs

#### 6.1.1 Classification des Types d'√âchec

**√âchecs de logique applicative :**
```python
# Test √©choue car logique m√©tier incorrecte
def test_universe_visibility_logic(client, test_universe):
    # Sc√©nario: Univers rendu priv√© devrait dispara√Ætre des publics
    slug = test_universe["slug"]

    # Action: Rendre priv√©
    client.patch(f"/api/universes/{slug}", json={"is_public": False})

    # V√©rification: Ne devrait plus appara√Ætre dans la liste publique
    response = client.get("/api/universes?is_public=true")
    public_universes = response.json()["items"]

    # √âchec si l'univers priv√© appara√Æt encore
    assert not any(u["slug"] == slug for u in public_universes)
```

**√âchecs de configuration :**
```python
# Test √©choue car d√©pendances non configur√©es
def test_replicate_integration():
    from services.generation_service import GenerationService
    service = GenerationService()

    # √âchec si Replicate non configur√©
    assert service.is_available, "Replicate API token not configured"
```

**√âchecs de donn√©es de test :**
```python
# Test √©choue car fixtures cr√©ent des conflits
def test_concurrent_universe_creation(client):
    # Cr√©ation simultan√©e peut causer des conflits de slugs
    responses = []
    for i in range(3):
        response = client.post("/api/universes", json={
            "name": f"Concurrent Universe {i}",
            "slug": f"concurrent-{i}"  # Risque de collision
        })
        responses.append(response)

    # Toutes les cr√©ations devraient r√©ussir
    assert all(r.status_code == 201 for r in responses)
```

#### 6.1.2 Analyse des Logs et Traces

**Pattern de d√©bogage structur√© :**

```python
import logging
from tests.utils.helpers import capture_logs

def test_detailed_debugging(client, test_universe):
    """Test avec logging d√©taill√© pour diagnostic."""

    with capture_logs('fastapi', 'sqlalchemy', 'uvicorn') as logs:
        # Action √† tester
        response = client.post(f"/api/universes/{test_universe['slug']}/music-prompts", json={
            "language": "fr",
            "prompt": "test prompt",
            "lyrics": "test lyrics"
        })

        # En cas d'√©chec, analyser les logs
        if response.status_code != 201:
            print("=== LOGS CAPTUR√âS ===")
            for log_entry in logs:
                print(f"{log_entry.levelname}: {log_entry.message}")

            print("=== R√âPONSE D√âTAILL√âE ===")
            print(f"Status: {response.status_code}")
            print(f"Headers: {dict(response.headers)}")
            print(f"Body: {response.text}")

            # Assertions d√©taill√©es
            if response.status_code == 422:
                errors = response.json()["detail"]
                for error in errors:
                    print(f"Validation error: {error}")

            # Ne pas faire l'assertion finale pour permettre l'analyse
            pytest.fail(f"Test failed with status {response.status_code}")
```

### 6.2 Outils de D√©bogage Avanc√©s

#### 6.2.1 Time Travel Debugging pour Tests

```python
from freezegun import freeze_time
import pytest

@freeze_time("2024-01-15 12:00:00")
def test_universe_creation_timestamp(client):
    """Test de cr√©ation avec timestamp contr√¥l√©."""

    response = client.post("/api/universes", json={
        "name": "Timestamp Test Universe",
        "slug": "timestamp-test"
    })

    assert response.status_code == 201
    universe = response.json()

    # V√©rifier que le timestamp correspond exactement
    assert universe["created_at"] == "2024-01-15T12:00:00"
```

#### 6.2.2 Mocking Intelligent avec Side Effects

```python
from unittest.mock import Mock, call

def test_music_generation_with_side_effects(client, test_universe):
    """Test avec mock qui simule des effets de bord."""

    slug = test_universe["slug"]

    # Mock avec historique d'appels
    mock_replicate = Mock()
    mock_replicate.run.side_effect = [
        # Premier appel: g√©n√©ration de musique
        ["audio_data_1"],
        # Deuxi√®me appel: traitement du fichier (si applicable)
        None
    ]

    with patch('services.generation_service.replicate.run', mock_replicate):
        response = client.post(f"/api/generate/{slug}/music", json={
            "language": "fr"
        })

        assert response.status_code == 200

        # V√©rifier l'historique des appels
        expected_calls = [
            call(
                "andreasjansson/llama-2-7b-chat-hf",  # Mod√®le utilis√©
                input={
                    "prompt": mock.ANY,  # Contient le prompt fran√ßais
                    "max_new_tokens": 500,
                    "temperature": 0.7
                }
            )
        ]

        mock_replicate.run.assert_has_calls(expected_calls)
```

#### 6.2.3 Fixtures de D√©bogage Interactif

```python
@pytest.fixture
def debug_session():
    """Session de debug interactive pour tests."""

    class DebugSession:
        def __init__(self):
            self.requests = []
            self.responses = []
            self.errors = []

        def log_request(self, method, url, data=None):
            self.requests.append({
                "method": method,
                "url": url,
                "data": data,
                "timestamp": time.time()
            })

        def log_response(self, status_code, response_data):
            self.responses.append({
                "status": status_code,
                "data": response_data,
                "timestamp": time.time()
            })

        def log_error(self, error_type, message, context=None):
            self.errors.append({
                "type": error_type,
                "message": message,
                "context": context,
                "timestamp": time.time()
            })

        def report(self):
            """G√©n√®re un rapport de debug."""
            print("=== DEBUG SESSION REPORT ===")
            print(f"Requests: {len(self.requests)}")
            print(f"Responses: {len(self.responses)}")
            print(f"Errors: {len(self.errors)}")

            if self.errors:
                print("\\n=== ERRORS ===")
                for error in self.errors:
                    print(f"{error['type']}: {error['message']}")

    return DebugSession()

@pytest.fixture
def debug_client(client, debug_session):
    """Client HTTP avec logging de debug."""

    # Monkey patch les m√©thodes du client
    original_post = client.post
    original_get = client.get
    original_patch = client.patch
    original_delete = client.delete

    def logged_post(url, **kwargs):
        debug_session.log_request("POST", url, kwargs.get("json"))
        response = original_post(url, **kwargs)
        debug_session.log_response(response.status_code, response.json() if response.content else None)
        return response

    def logged_get(url, **kwargs):
        debug_session.log_request("GET", url, kwargs.get("params"))
        response = original_get(url, **kwargs)
        debug_session.log_response(response.status_code, response.json() if response.content else None)
        return response

    # Appliquer les patches
    client.post = logged_post
    client.get = logged_get
    client.patch = logged_patch if hasattr(client, 'patch') else None
    client.delete = logged_delete if hasattr(client, 'delete') else None

    # Injecter la session de debug
    client.debug_session = debug_session

    yield client

    # Rapport final
    debug_session.report()
```

### 6.3 M√©triques et Qualit√© des Tests

#### 6.3.1 Dashboard de Qualit√©

```python
# tests/utils/metrics.py
class TestMetricsCollector:
    """Collecteur de m√©triques de test."""

    def __init__(self):
        self.metrics = {
            "tests_run": 0,
            "tests_passed": 0,
            "tests_failed": 0,
            "execution_time": 0,
            "coverage": 0,
            "slow_tests": [],
            "flaky_tests": []
        }

    def collect_pytest_metrics(self, session):
        """Collecte les m√©triques depuis la session pytest."""
        self.metrics.update({
            "tests_run": session.testscollected,
            "tests_passed": session.tests_passed,
            "tests_failed": session.tests_failed,
            "execution_time": time.time() - session.starttime
        })

    def analyze_test_performance(self, test_results):
        """Analyse les performances des tests."""
        for test_name, duration in test_results.items():
            if duration > 5.0:  # Test lent
                self.metrics["slow_tests"].append({
                    "name": test_name,
                    "duration": duration
                })

    def generate_report(self):
        """G√©n√®re un rapport de qualit√©."""
        success_rate = (self.metrics["tests_passed"] / self.metrics["tests_run"]) * 100

        report = f"""
=== RAPPORT DE QUALIT√â DES TESTS ===

Tests ex√©cut√©s: {self.metrics["tests_run"]}
Taux de succ√®s: {success_rate:.1f}%
Temps d'ex√©cution: {self.metrics["execution_time"]:.2f}s
Tests lents (>5s): {len(self.metrics["slow_tests"])}

Recommandations:
"""

        if success_rate < 90:
            report += "- Am√©liorer la stabilit√© des tests\\n"

        if len(self.metrics["slow_tests"]) > 5:
            report += "- Optimiser les tests lents\\n"

        if self.metrics["execution_time"] > 300:  # 5 minutes
            report += "- Parall√©liser l'ex√©cution des tests\\n"

        return report

@pytest.fixture(scope="session", autouse=True)
def test_metrics(request):
    """Fixture globale pour collecter les m√©triques."""
    collector = TestMetricsCollector()

    # Injecter dans la session pytest
    request.session.test_metrics = collector

    yield collector

    # G√©n√©rer le rapport final
    report = collector.generate_report()
    print(report)
```

#### 6.3.2 Tests de Non-R√©gression

```python
# tests/regression/test_historical_bugs.py
class TestRegressionSuite:
    """Suite de tests pour pr√©venir les r√©gressions."""

    def test_universe_slug_validation_regression(self, client):
        """Test de r√©gression pour validation des slugs.

        Bug historique: Les slugs avec caract√®res sp√©ciaux √©taient accept√©s.
        Corrig√© le: 2024-01-15
        """
        invalid_slugs = [
            "test@universe",    # @ accept√© √† tort
            "test universe",    # espaces accept√©s √† tort
            "test/universe",    # / accept√© √† tort
        ]

        for invalid_slug in invalid_slugs:
            response = client.post("/api/universes", json={
                "name": "Test Universe",
                "slug": invalid_slug,
                "is_public": True
            })

            # Doit maintenant √™tre rejet√©
            assert response.status_code == 422

    def test_music_prompt_uniqueness_regression(self, client, test_universe):
        """Test de r√©gression pour unicit√© des prompts musicaux.

        Bug historique: Plusieurs prompts pour m√™me langue autoris√©s.
        Corrig√© le: 2024-01-15
        """
        slug = test_universe["slug"]

        # Cr√©er le premier prompt fran√ßais
        data = {
            "language": "fr",
            "prompt": "Musique douce",
            "lyrics": "Paroles fran√ßaises"
        }
        response1 = client.post(f"/api/universes/{slug}/music-prompts", json=data)
        assert response1.status_code == 201

        # Tenter un deuxi√®me prompt fran√ßais - doit √©chouer
        response2 = client.post(f"/api/universes/{slug}/music-prompts", json=data)
        assert response2.status_code == 400
        assert "already exists" in response2.json()["detail"]
```

2. **Status Code** : API retourne pas le bon code
```python
# Erreur: assert 201 == 400
# Fix: V√©rifier la validation des donn√©es
```

3. **KeyError** : Cl√© manquante dans r√©ponse
```python
# Erreur: response["missing_key"]
# Fix: V√©rifier la structure de r√©ponse
```

#### **Debug Step-by-Step**

```python
def test_debugging_example(client):
    # √âtape 1: V√©rifier les donn√©es d'entr√©e
    data = {"name": "Test"}
    print(f"Input data: {data}")

    # √âtape 2: Faire l'appel
    response = client.post("/api/test", json=data)
    print(f"Status: {response.status_code}")

    # √âtape 3: Examiner la r√©ponse
    if response.status_code != 200:
        print(f"Error response: {response.text}")
        return

    result = response.json()
    print(f"Response: {result}")

    # √âtape 4: Assertions
    assert result["name"] == "Test"
```

### üõ†Ô∏è Outils de Debug

#### **1. Print Debugging**
```python
def test_with_debug(client):
    response = client.get("/api/test")
    print(f"DEBUG: Status={response.status_code}")
    print(f"DEBUG: Response={response.json()}")
    assert response.status_code == 200
```

#### **2. Pytest Options**
```bash
# Mode verbose pour plus de d√©tails
pytest -v

# Stop au premier √©chec
pytest -x

# Debug interactif
pytest --pdb
```

#### **3. Fixtures de Debug**
```python
@pytest.fixture
def debug_client(client):
    """Client avec logs de debug."""
    original_request = client._client.request

    def logged_request(*args, **kwargs):
        print(f"REQUEST: {args} {kwargs}")
        response = original_request(*args, **kwargs)
        print(f"RESPONSE: {response.status_code}")
        return response

    client._client.request = logged_request
    return client
```

---

## Chapitre VII : Synth√®se et Perspectives

### 7.1 √âvaluation de la Maturit√© de Test

#### 7.1.1 M√©triques de Qualit√© pour MagikSwipe

**Couverture fonctionnelle actuelle :**
- ‚úÖ **CRUD Univers** : 100% (6/6 endpoints test√©s)
- ‚úÖ **Gestion Visibilit√©** : 100% (filtrage + toggle)
- ‚úÖ **CRUD Assets** : 100% (avec traductions)
- ‚úÖ **CRUD Music Prompts** : 91% (10/11 tests r√©ussis)
- ‚úÖ **Jobs & Sync** : 86% (6/7 tests r√©ussis)
- ‚úÖ **G√©n√©ration IA** : 60% (avec mocks strat√©giques)

**Score global : 47/51 tests r√©ussis (92%)**

#### 7.1.2 Analyse SWOT des Tests

**Forces (Strengths) :**
- Architecture de test robuste avec fixtures hi√©rarchiques
- Utilisation strat√©gique des mocks pour √©viter les co√ªts
- Tests d'int√©gration complets pour l'API REST
- Couverture des sc√©narios m√©tier critiques

**Faiblesses (Weaknesses) :**
- D√©pendance aux services externes (Replicate, Supabase)
- Tests E2E limit√©s aux workflows critiques
- Complexit√© de maintenance des fixtures avanc√©es

**Opportunit√©s (Opportunities) :**
- Extension vers tests de performance
- Int√©gration CI/CD compl√®te
- Tests de s√©curit√© avanc√©s
- Monitoring de la qualit√© de code

**Menaces (Threats) :**
- √âvolution des APIs externes (Replicate, Supabase)
- Augmentation des co√ªts si mocks insuffisants
- Complexit√© croissante de l'architecture

### 7.2 Roadmap d'Am√©lioration

#### 7.2.1 Phase 1 : Consolidation (1-2 semaines)
```python
# Objectifs prioritaires
- [ ] Corriger les 4 tests en √©chec restants
- [ ] Stabiliser les mocks pour g√©n√©ration IA
- [ ] Documenter les patterns de test √©tablis
- [ ] Cr√©er des templates pour nouveaux tests
```

#### 7.2.2 Phase 2 : Extension (2-4 semaines)
```python
# Extensions fonctionnelles
- [ ] Tests de performance (locust ou pytest-benchmark)
- [ ] Tests de s√©curit√© (OWASP ZAP ou √©quivalent)
- [ ] Tests d'accessibilit√© pour l'interface utilisateur
- [ ] Tests de charge pour les endpoints critiques
```

#### 7.2.3 Phase 3 : Industrialisation (1-2 mois)
```python
# Automatisation et monitoring
- [ ] Pipeline CI/CD complet avec tests parall√®les
- [ ] Dashboard de m√©triques de qualit√© (SonarQube)
- [ ] Tests de mutation (mutmut ou cosmic-ray)
- [ ] Int√©gration avec outils de revue de code
```

### 7.3 Bonnes Pratiques √âtablies

#### 7.3.1 Patterns Recommand√©s pour l'√âquipe

**Structure de test normalis√©e :**
```python
def test_feature_scenario_expected_result(client, fixtures):
    """Test descriptif avec Given-When-Then implicite.

    Given: Pr√©conditions √©tablies par les fixtures
    When: Action ex√©cut√©e (call API)
    Then: Assertions sur le r√©sultat attendu
    """
    # Arrange (pr√©par√© par fixtures)
    # Act (appel API)
    # Assert (v√©rifications)
```

**Gestion des donn√©es de test :**
- Utiliser des factories pour g√©n√©rer des donn√©es uniques
- Pr√©f√©rer les fixtures aux setups/teardowns manuels
- Isoler les tests pour √©viter les interf√©rences

**Strat√©gie de mocking :**
- Mock les services externes co√ªteux (Replicate)
- Pr√©f√©rer les mocks de bas niveau aux int√©grations r√©elles
- Documenter les sc√©narios mock√©s

#### 7.3.2 R√®gles de D√©veloppement Guid√© par les Tests

**R√®gle 1 : Test First**
```
√âcrivez le test AVANT d'impl√©menter la fonctionnalit√©.
Le test d√©finit le contrat de l'API.
```

**R√®gle 2 : Tests Ind√©pendants**
```
Chaque test doit pouvoir s'ex√©cuter isol√©ment.
Pas d'ordre d'ex√©cution impos√©.
```

**R√®gle 3 : Assertions Pr√©cises**
```
Une assertion = un concept v√©rifi√©.
Pr√©f√©rer les assertions sp√©cifiques aux v√©rifications g√©n√©riques.
```

**R√®gle 4 : Maintenance Continue**
```
Les tests √©voluent avec le code.
Supprimer les tests obsol√®tes.
Refactorer les tests comme le code.
```

### 7.4 R√©f√©rences et Lectures Recommand√©es

#### 7.4.1 Ouvrages Fondamentaux

- **"Test-Driven Development: By Example"** - Kent Beck
  - R√©f√©rence historique du TDD
  - Exemples pratiques en Java/Smalltalk

- **"Growing Object-Oriented Software, Guided by Tests"** - Steve Freeman, Nat Pryce
  - Approche TDD pour syst√®mes complexes
  - Patterns avanc√©s d'isolation et mocking

- **"Clean Code"** - Robert C. Martin
  - Chapitre 9 d√©di√© aux tests automatis√©s
  - Principes de qualit√© pour le code de test

#### 7.4.2 Ressources Sp√©cialis√©es FastAPI

- **"FastAPI Testing Documentation"** - https://fastapi.tiangolo.com/tutorial/testing/
- **"Pytest Best Practices"** - https://docs.pytest.org/en/stable/
- **"Testing in Python"** - Real Python (https://realpython.com/python-testing/)

#### 7.4.3 Communaut√© et √âchanges

- **Forum 42** : √âchanges avec pairs sur les pratiques de test
- **Stack Overflow** : Questions techniques sp√©cifiques
- **Reddit r/learnpython** : Discussions sur les frameworks de test
- **PyTest Discord** : Support communautaire pour pytest

---

## √âpilogue : L'Art du Test-Driven Development

La mise en place d'une suite de tests automatis√©s repr√©sente bien plus qu'une pratique technique : c'est une philosophie de d√©veloppement qui garantit la fiabilit√©, la maintenabilit√© et l'√©volutivit√© des syst√®mes logiciels.

Dans le contexte de MagikSwipe, cette approche a permis de valider :

- **L'int√©grit√© fonctionnelle** de l'API REST
- **La robustesse des workflows m√©tier** complexes
- **La stabilit√© des int√©grations externes**
- **La qualit√© de l'architecture logicielle**

**Perspectives d'√©volution :**
L'excellence en testing est un voyage continu. Chaque nouveau d√©fi technique enrichit la bo√Æte √† outils, chaque bug d√©couvert affine les strat√©gies de pr√©vention. La maturit√© atteinte avec MagikSwipe constitue une base solide pour aborder des probl√©matiques plus complexes : microservices, architectures distribu√©es, intelligence artificielle.

**Recommandation finale :**
Int√©grez les tests dans votre ADN de d√©veloppement. Ils ne sont pas une contrainte, mais le garant de votre libert√© cr√©ative et de la confiance dans vos r√©alisations.

*"Code without tests is broken by design."*

---

**Annexe A : Glossaire des Termes Techniques**

- **Fixture** : Fonction pr√©parant l'environnement de test
- **Mock** : Objet simulant un comportement pour isoler les tests
- **Stub** : Impl√©mentation simplifi√©e retournant des valeurs pr√©d√©finies
- **Spy** : Mock enregistrant les interactions pour v√©rification
- **Factory** : Fonction g√©n√©rant des donn√©es de test r√©alistes
- **Assertion** : V√©rification que le comportement observ√© correspond √† l'attendu

**Annexe B : Configuration pytest Optimis√©e**

```ini
# pytest.ini - Configuration recommand√©e
[tool:pytest.ini_options]
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*
addopts =
    --strict-markers
    --disable-warnings
    --tb=short
    --cov=backend
    --cov-report=html
    --cov-report=term-missing
    --cov-fail-under=85
markers =
    unit: Tests unitaires rapides
    integration: Tests d'int√©gration
    e2e: Tests end-to-end
    slow: Tests n√©cessitant plus de 5 secondes
    api: Tests des endpoints API
    generation: Tests de g√©n√©ration IA
```

---

*Manuel de r√©f√©rence pour les tests automatis√©s dans MagikSwipe. Version 1.0 - Janvier 2024.*